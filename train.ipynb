{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**\n",
    "#  * @file train.ipynb\n",
    "#  * @author Samay Pashine (samay@iiti.ac.in)\n",
    "#  * @modified Samay Pashine (samay@iiti.ac.in)\n",
    "#  * @brief Train the neural network model to predict yield on crop outputs, soil and climate basis.\n",
    "#  * @version 2.0\n",
    "#  * @date 2021-11-12\n",
    "#  * @copyright Copyright (c) 2021\n",
    "#  */\n",
    "\n",
    "# Importing necessary libraries.\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.ipc as ipc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import config\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_growth_GPU():\n",
    "    \"\"\" Enabling memory growth in GPU (if present) for training the model. \"\"\"\n",
    "    try:\n",
    "        physicalDevices = config.experimental.list_physical_devices('GPU')\n",
    "        config.experimental.set_memory_growth(physicalDevices[0], True)\n",
    "    except:\n",
    "        print(\"[ERR]. Could not enable the memory growth in GPU. Switching to CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feather_in_chunks(filepath):\n",
    "    \"\"\"Function to read feather file in chunks instead of all at once.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path of final_input feather file.\n",
    "\n",
    "    Yields:\n",
    "        data_df [pandas.DataFrame]: return pandas Dataframe from the feather file.\n",
    "    \"\"\"\n",
    "    with ipc.RecordBatchFileReader(filepath) as reader:\n",
    "        for batch_index in range(reader.num_record_batches):\n",
    "            if batch_index == 0:\n",
    "                batch = reader.get_batch(batch_index).to_pandas(use_threads=True, timestamp_as_object=True, )\n",
    "            else:\n",
    "                new_batch = reader.get_batch(batch_index).to_pandas(use_threads=True, timestamp_as_object=True, )\n",
    "                data_df = pd.concat([batch, new_batch], ignore_index=True)\n",
    "                batch = data_df\n",
    "            \n",
    "            # Instead of taking just one batch with 65,000 rows (approx.), \n",
    "            # we let the loop iterate over batches until it triggers the condition below.\n",
    "            if (batch_index + 1) % 2 == 0:\n",
    "                batch = pd.DataFrame()\n",
    "                yield data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"  This is the driver code which initializes all the variable, trains the model and save the outputs. \"\"\"\n",
    "    \n",
    "    # Calling the function to switch processing to GPU (if present).\n",
    "    memory_growth_GPU()\n",
    "    \n",
    "    # Initializing variables.\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-7\n",
    "    BATCH_SIZE = 64\n",
    "    ES_PATIENCE = 3\n",
    "    VAL_SPLIT = 0.2\n",
    "    SEQUENCE = 1\n",
    "    flag = 1\n",
    "    \n",
    "    for input_file in os.listdir(os.path.join(input_dir, final_inputs_dir)):\n",
    "        \"\"\" Loop to iterate through all the input files in the directory for training.\n",
    "        \"\"\"\n",
    "        batch_num = 1\n",
    "        \n",
    "        # Condition to check if the graph directory for the input_file exists. If not, then create one.\n",
    "        if not os.path.isdir(os.path.join(output_dir, graphs_dir, input_file[:-8] + '_S-' + str(SEQUENCE))):\n",
    "            try:\n",
    "                print(\"[INFO]. Graph directory for the input file \\'{}\\' does not exists. Creating the directory.\".format(input_file))\n",
    "                os.makedirs(os.path.join(output_dir, graphs_dir, input_file[:-8] + '_S-' + str(SEQUENCE)))\n",
    "                print(\"[INFO]. Directory created successfully.\")\n",
    "            except:\n",
    "                print(\"[WARNING]. Directory for the input file \\'{}\\' already exists.\".format(input_file))\n",
    "        \n",
    "        for batch in read_feather_in_chunks(os.path.join(input_dir, final_inputs_dir, input_file)):\n",
    "            \"\"\" Loop to iterate through batches in the input feather files. \"\"\"\n",
    "            \n",
    "            # Condition to check if the 'saving model' directory for the input_file exists. If not, then create one.\n",
    "            if not os.path.isdir(os.path.join(output_dir, saved_models_dir, input_file[:-8] + '_S-' + str(SEQUENCE))):\n",
    "                try:\n",
    "                    print(\"[INFO]. Saving model directory for the input file \\'{}\\' does not exists. Creating the directory.\".format(input_file))\n",
    "                    os.makedirs(os.path.join(output_dir, saved_models_dir, input_file[:-8] + '_S-' + str(SEQUENCE), str(batch_num) + '_batch'))\n",
    "                    print(\"[INFO]. Directory created successfully.\")\n",
    "                except:\n",
    "                    print(\"[WARNING]. Directory for the input file \\'{}\\' already exists.\".format(input_file))\n",
    "\n",
    "            # Loop to calculate the tasmax, tasmin and precipitation_flux in the batch.\n",
    "            print(\"[INFO]. Pre-Processing Batch-{} Inputs.\".format(batch_num))\n",
    "            for i in tqdm(range(len(batch))):\n",
    "                batch.iloc[i, 25] += batch.iloc[i, 11]\n",
    "                batch.iloc[i, 26] += batch.iloc[i, 11]    \n",
    "                if batch.iloc[i, 10] != np.inf:\n",
    "                    batch.iloc[i, 27] = (1 + batch.iloc[i, 10] / 100) * batch.iloc[i, 27]\n",
    "            \n",
    "            # Final formatting of the dataframe before traning.\n",
    "            batch = batch.drop(columns=['index', 'time', 'lat', 'lon', 'index_x', 'index_y', 'spatial_ref', 'W', 'T'])\n",
    "            batch.gravel = batch.gravel.astype(int)\n",
    "            batch.clay = batch.clay.astype(int)\n",
    "            batch.silt = batch.silt.astype(int)\n",
    "            batch.sand = batch.sand.astype(int)\n",
    "            batch.awc = batch.awc.astype(int)\n",
    "            batch.cec_soil = batch.cec_soil.astype(int)\n",
    "            batch.texture_class = batch.texture_class.astype(int)\n",
    "            batch.CO2 = batch.CO2.astype(int)\n",
    "            batch['plant-day'] = batch['plant-day'].astype(int)\n",
    "            batch['maturity-day'] = batch['maturity-day'].astype(int)\n",
    "\n",
    "            # Dividing the dataframe in static and dynamic dataframe on the basis of features.\n",
    "            static_data_input = batch[['plant-day', 'maturity-day', 'CO2', 'N', 'A', 'texture_class', 'soil_ph',\n",
    "                                        'soil_caco3', 'cec_soil', 'oc', 'awc', 'sand', 'silt', 'clay', 'gravel']]\n",
    "            static_data_label = batch[['yield_mai']]\n",
    "            weather_array_1 = batch[['tasmax', 'tasmin', 'pr', 'gdd']]\n",
    "            \n",
    "            # Scaling static and dynamic data to assist in the training.\n",
    "            scaler = MinMaxScaler(feature_range=(0.01, 1))\n",
    "            scaled_static_data = scaler.fit_transform(static_data_input)\n",
    "            scaled_static_label = scaler.fit_transform(static_data_label)\n",
    "            scaled_dynamic_data = scaler.fit_transform(weather_array_1)\n",
    "            \n",
    "            # Clear the memory buffer and deleting un-necessary variables.\n",
    "            gc.collect()\n",
    "            del batch, static_data_input, static_data_label, weather_array_1\n",
    "            \n",
    "            # Splitting the static and dynamic dataframe in training and testing set.\n",
    "            test_size = 0.2\n",
    "            fract = 1 - test_size\n",
    "\n",
    "            static_X_train = scaled_static_data[:int(len(scaled_static_data) * fract)]\n",
    "            static_X_test = scaled_static_data[int(len(scaled_static_data) * fract):]\n",
    "\n",
    "            static_Y_train = scaled_static_label[:int(len(scaled_static_label) * fract)]\n",
    "            static_Y_test = scaled_static_label[int(len(scaled_static_label) * fract):]\n",
    "\n",
    "            dynamic_X_train = scaled_dynamic_data[:int(len(scaled_dynamic_data) * fract)]\n",
    "            dynamic_X_test = scaled_dynamic_data[int(len(scaled_dynamic_data) * fract):]\n",
    "\n",
    "            # Clear the memory buffer and deleting un-necessary variables.\n",
    "            gc.collect()\n",
    "            del scaled_static_data, scaled_static_label, scaled_dynamic_data\n",
    "\n",
    "            # Defining the neural network for training the model.\n",
    "            if flag == 1:\n",
    "                dynamic_input = keras.Input(shape = (dynamic_X_train.shape[1], 1), dtype='float32')\n",
    "                inner_lstm1 = LSTM(200, return_sequences=True)(dynamic_input)\n",
    "                inner_lstm2 = LSTM(200, return_sequences=True)(inner_lstm1)\n",
    "                lstm_out = LSTM(200, return_sequences=False)(inner_lstm2)\n",
    "\n",
    "                static_input = keras.Input(shape = (static_X_train.shape[1]))\n",
    "                inner_stat1 = Dense(200, activation='selu')(static_input)\n",
    "                inner_stat1 = Dense(200, activation='selu')(inner_stat1)\n",
    "                inner_stat2 = Dense(200, activation='selu')(inner_stat1)     \n",
    "\n",
    "                x = Concatenate()([lstm_out, inner_stat2])\n",
    "\n",
    "                x = Dense(200, activation='selu')(x)\n",
    "                x = Dense(200, activation='selu')(x)\n",
    "                x = Dense(200, activation='selu')(x)\n",
    "\n",
    "                dynamic_output = Dense(1, activation = 'selu')(x)\n",
    "\n",
    "                model = Model(inputs = [dynamic_input, static_input], outputs = [dynamic_output])\n",
    "\n",
    "                model.compile(loss = keras.metrics.mean_squared_error,\n",
    "                            optimizer = optimizers.Adam(learning_rate = LEARNING_RATE),\n",
    "                            metrics = [keras.metrics.RootMeanSquaredError(name = 'rmse'), 'mae'])\n",
    "\n",
    "                logs = \"./logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "                tboard_callback = keras.callbacks.TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = '500,520')\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    model = keras.models.load_model(os.path.join(output_dir, saved_models_dir, input_file[:-8] + '_S-' + str(SEQUENCE), str(batch_num-1)+'_batch'))\n",
    "                except:\n",
    "                    print(\"[INFO]. Input File has been completed. Moving onto the new input file.\")\n",
    "                    model = keras.models.load_model(prev_model)\n",
    "\n",
    "                es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = ES_PATIENCE)\n",
    "                tboard_callback = keras.callbacks.TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = '500,520')\n",
    "\n",
    "            # Training the mode on the dataset.\n",
    "            history = model.fit(x = [dynamic_X_train, static_X_train], y = static_Y_train, validation_split = VAL_SPLIT, epochs = EPOCHS, callbacks = [tboard_callback, es], batch_size = BATCH_SIZE)\n",
    "\n",
    "            # Plottting the loss graph and saving it in graph directory.\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.savefig(os.path.join(output_dir, graphs_dir, input_file[:-8], \"Batch-{}_loss_stats.jpg\".format(batch_num)))\n",
    "            plt.clf()\n",
    "\n",
    "            # Saving the model after each epoch in corresponding directory..\n",
    "            model.save(os.path.join(output_dir, saved_models_dir, input_file[:-8] + '_S-' + str(SEQUENCE), str(batch_num)+'_batch'))\n",
    "            \n",
    "            # Condition to handle when one input_file is completed and about to switch to another one.\n",
    "            if batch_num == 12:\n",
    "                prev_model = os.path.join(output_dir, saved_models_dir, input_file[:-8] + '_S-' + str(SEQUENCE), str(batch_num) + '_batch')\n",
    "            \n",
    "            # Clearing the memory buffer and incrementing the variables.\n",
    "            gc.collect()\n",
    "            flag += 1\n",
    "            batch_num += 1\n",
    "    \n",
    "    SEQUENCE += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
