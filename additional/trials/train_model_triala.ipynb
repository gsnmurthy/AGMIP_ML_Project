{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow import config\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pyarrow.ipc as ipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicalDevices = config.experimental.list_physical_devices('GPU')\n",
    "config.experimental.set_memory_growth(physicalDevices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feather_in_chunks(filepath):\n",
    "    with ipc.RecordBatchFileReader(filepath) as reader:\n",
    "        print(reader.num_record_batches)\n",
    "        for batch_index in range(reader.num_record_batches):\n",
    "            # print(\"[INFO]. Reading batch-{} which had {} rows\".format(batch_index, batch.num_rows))\n",
    "            if batch_index == 0:\n",
    "                batch = reader.get_batch(batch_index).to_pandas(use_threads=True, timestamp_as_object=True, )\n",
    "            else:\n",
    "                new_batch = reader.get_batch(batch_index).to_pandas(use_threads=True, timestamp_as_object=True, )\n",
    "                data_df = pd.concat([batch, new_batch], ignore_index=True)\n",
    "                batch = data_df\n",
    "            \n",
    "            if (batch_index + 1) % 2 == 0:\n",
    "                batch = pd.DataFrame()\n",
    "                yield data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 1\n",
    "path = \"./Inputs/final_inputs/\"\n",
    "for input_file in os.listdir(path):\n",
    "    batch_num = 1\n",
    "    os.makedirs(os.path.join(\"./graphs\", input_file[:-8]))\n",
    "    for batch in read_feather_in_chunks(os.path.join(path, input_file)):\n",
    "        os.makedirs(os.path.join(\"./saved_model\", input_file[:-8], str(batch_num)+'_batch'))\n",
    "\n",
    "        print(\"[INFO]. Pre-Processing Batch-{} Inputs.\".format(batch_num))\n",
    "        for i in tqdm(range(len(batch))):\n",
    "            batch.iloc[i, 25] += batch.iloc[i, 11]\n",
    "            batch.iloc[i, 26] += batch.iloc[i, 11]    \n",
    "            if batch.iloc[i, 10] != np.inf:\n",
    "                batch.iloc[i, 27] = (1 + batch.iloc[i, 10] / 100) * batch.iloc[i, 27]  # (1 + W/100) * pr) W = 20%, pr = 0.0027\n",
    "        \n",
    "        batch = batch.drop(columns=['index', 'time', 'lat', 'lon', 'index_x', 'index_y', 'spatial_ref', 'W', 'T'])\n",
    "        batch.gravel = batch.gravel.astype(int)\n",
    "        batch.clay = batch.clay.astype(int)\n",
    "        batch.silt = batch.silt.astype(int)\n",
    "        batch.sand = batch.sand.astype(int)\n",
    "        batch.awc = batch.awc.astype(int)\n",
    "        batch.cec_soil = batch.cec_soil.astype(int)\n",
    "        batch.texture_class = batch.texture_class.astype(int)\n",
    "        batch.CO2 = batch.CO2.astype(int)\n",
    "        batch['plant-day'] = batch['plant-day'].astype(int)\n",
    "        batch['maturity-day'] = batch['maturity-day'].astype(int)\n",
    "\n",
    "        static_data_input = batch[['plant-day', 'maturity-day', 'CO2', 'N', 'A', 'texture_class', 'soil_ph',\n",
    "                                    'soil_caco3', 'cec_soil', 'oc', 'awc', 'sand', 'silt', 'clay', 'gravel']]\n",
    "        static_data_label = batch[['yield_mai']]\n",
    "        weather_array_1 = batch[['tasmax', 'tasmin', 'pr', 'gdd']]\n",
    "        gc.collect()\n",
    "        del batch\n",
    "\n",
    "        # Scaling static and dynamic data.\n",
    "        scaler = MinMaxScaler(feature_range=(0.01, 1))\n",
    "        scaled_static_data = scaler.fit_transform(static_data_input)\n",
    "        scaled_static_label = scaler.fit_transform(static_data_label)\n",
    "        scaled_dynamic_data = scaler.fit_transform(weather_array_1)\n",
    "        # scaled_static_data = static_data_input\n",
    "        # scaled_static_label = static_data_label\n",
    "        # scaled_dynamic_data = weather_array_1\n",
    "        gc.collect()\n",
    "        del static_data_input, static_data_label, weather_array_1\n",
    "        \n",
    "        # Splitting the batch in training and testing set.\n",
    "        test_size = 0.2\n",
    "        fract = 1 - test_size\n",
    "\n",
    "        static_X_train = scaled_static_data[:int(len(scaled_static_data) * fract)]\n",
    "        static_X_test = scaled_static_data[int(len(scaled_static_data) * fract):]\n",
    "\n",
    "        static_Y_train = scaled_static_label[:int(len(scaled_static_label) * fract)]\n",
    "        static_Y_test = scaled_static_label[int(len(scaled_static_label) * fract):]\n",
    "\n",
    "        dynamic_X_train = scaled_dynamic_data[:int(len(scaled_dynamic_data) * fract)]\n",
    "        dynamic_X_test = scaled_dynamic_data[int(len(scaled_dynamic_data) * fract):]\n",
    "\n",
    "        gc.collect()\n",
    "        del scaled_static_data, scaled_static_label, scaled_dynamic_data\n",
    "\n",
    "        # Defining the neural network for training the model.\n",
    "        if flag == 1:\n",
    "            dynamic_input = keras.Input(shape = (dynamic_X_train.shape[1], 1), dtype='float32')\n",
    "            inner_lstm1 = LSTM(200, return_sequences=True)(dynamic_input)\n",
    "            inner_lstm2 = LSTM(200, return_sequences=True)(inner_lstm1)\n",
    "            lstm_out = LSTM(200, return_sequences=False)(inner_lstm2)\n",
    "\n",
    "            static_input = keras.Input(shape = (static_X_train.shape[1]))\n",
    "            inner_stat1 = Dense(200, activation='selu')(static_input)\n",
    "            inner_stat1 = Dense(200, activation='selu')(inner_stat1)\n",
    "            inner_stat2 = Dense(200, activation='selu')(inner_stat1)     \n",
    "\n",
    "            x = Concatenate()([lstm_out, inner_stat2])\n",
    "\n",
    "            x = Dense(200, activation='selu')(x)\n",
    "            x = Dense(200, activation='selu')(x)\n",
    "            x = Dense(200, activation='selu')(x)\n",
    "\n",
    "            dynamic_output = Dense(1, activation = 'selu')(x)\n",
    "\n",
    "            model = Model(inputs = [dynamic_input, static_input], outputs = [dynamic_output])\n",
    "\n",
    "            model.compile(loss = keras.metrics.mean_squared_error,\n",
    "                        optimizer = optimizers.Adam(learning_rate = 1e-5),\n",
    "                        metrics = [keras.metrics.RootMeanSquaredError(name = 'rmse'), 'mae'])\n",
    "\n",
    "            logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "            tboard_callback = keras.callbacks.TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = '500,520')\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                model = keras.models.load_model(os.path.join(\"./saved_model\", input_file[:-8], str(batch_num-1)+'_batch'))\n",
    "            except:\n",
    "                print(\"[INFO]. Input File has been completed. Moving onto the new input file.\")\n",
    "                model = keras.models.load_model(prev_model)\n",
    "            es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "            tboard_callback = keras.callbacks.TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = '500,520')\n",
    "\n",
    "        # Training the mode on the dataset.\n",
    "        history = model.fit(x = [dynamic_X_train, static_X_train], y = static_Y_train, validation_split = 0.2, epochs = 20, callbacks = [tboard_callback, es], batch_size = 64)\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.savefig(os.path.join(\"./graphs\", input_file[:-8], \"Batch-{}_loss_stats.jpg\".format(batch_num)))\n",
    "        plt.clf()\n",
    "\n",
    "        # Saving the model after each epoch.\n",
    "        model.save(os.path.join(\"./saved_model\", input_file[:-8], str(batch_num)+'_batch'))\n",
    "        \n",
    "        if batch_num == 12:\n",
    "            prev_model = os.path.join(\"./saved_model\", input_file[:-8], str(batch_num)+'_batch')\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        flag += 1\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"static_X_train Shape : \", static_X_train.shape)\n",
    "print(\"y_train\", static_Y_train.shape)\n",
    "\n",
    "print(\"\\nstatic_X_test Shape  : \", static_X_test.shape)\n",
    "print(\"y_test\", static_Y_test.shape)\n",
    "\n",
    "print(\"\\ndynamic_X_train Shape : \", dynamic_X_train.shape)\n",
    "print(\"dynamic_X_test Shape  : \", dynamic_X_test.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
